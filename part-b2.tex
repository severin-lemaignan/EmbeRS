\newrefsection
\chapter{B2.a State-of-the-art and objectives}

\TODO{SECTION B2.a IS HEAVILY WORK IN PROGRESS! many blocks of text are missing
or not at the right place}


\eu{(B2.a, B2.b, B2.c: max 15 pages (2 pages for B2.c)}
\eu{Specify the proposal objectives in the context of the state
of the art in the research field. It should be clear how and why the proposed work is important for
the field, and what impact it will have if successful, such as how it may open up new horizons or
opportunities for science, technology or scholarship. Specify any particularly challenging or
unconventional aspects of the proposal, including multi- or inter-disciplinary aspects.}

\TODO{as a reference: DECRESIM project: 4 pages on B2.a State of art and
objectives; B2.b ~7 pages on WPs + 2 pages on risk assessment}




\subsection{State of the art: real-world social robots and impact on the
society}



\subsubsection{Psycho-social underpinnings of social robotics}

\begin{figure}
    \centering
    \includegraphics[width=0.9\linewidth]{figs/wizme+dolls}
    \caption{Early prototyping for the \project project: a pet-like robot that could
    mediate child-child interactions}
    \label{}
\end{figure}


Developing a conceptual framework for \emph{robot-supported human-human
interactions} implies a deep understanding of the psycho-social mechanisms at
work in human-human interactions, and in human-robot interactions.

Anthropomorphism is a psychological mechanism by which a human
ascribe human traits to non-human artefacts (here, robots): while the
robot appearance and behaviour can elicit or reinforce these ascription,
it fundamentally originate in the human her/himself~\cite{fink}.





\subsubsection{The limits of real-world social robotics}

\TODO{rephrase section}

Social robots are a disruptive technology, poised to have a profound impact on
business, society, and the global economy~\cite{williams2020social}. A recent report from the United
Nations about the the impact of the technological revolution on labour markets
stated that AI and robotics are expected to radically change the labor market
world-wide destroying some job categories and creating others [32]. The impact
of AI applications and manufacturing robots on economies and labor markets is
already tangible. Yet, this is not the case for social robots despite the fact that
this technology is expected to have a significant impact on different application
areas such as care for the elderly, customer service, education, child development,
and autonomous vehicles~\cite{baillie2019challenges}.

As a matter of fact, in the past years promising companies are facing crises.
Some of them are being bought by multinationals, such as Aldebaran, Boston
Dynamics and Scharf, acquired by Softbank Group, while others shutdown (e.g.,
Willow Garage, Anki, Jibo). Developing and selling robots is challenging and
require market knowledge that companies born as a spin-off from Universities may
not have. This can be the case of Rethink Robotics founded by Rodney Brooks and
Jibo co-founded by Cynthia Breazeal, both of them from the Massachusetts
Institute of Technology. After releasing the collaborative robot Baxter and its
counterpart Sawyer, Rethink Robotics has been acquired by HAHN Group, a German
automation specialist. Jibo stopped operating nearly one year after it first
came to the market. The same end has been encountered by Anki, a robotics and
artificial intelligence startup founded by three graduated students from
Carnegie Mellon University.

The failures of companies like Jibo, Kuri, Willow Garage and Anki are examples
that bring some values to our reflection. Consumer robots may not be for
general-purpose, as a matter of fact the majority of robotics companies who
succeeds, design and build robots are meant for specific tasks: autonomous
cleaning devices (e.g., iRobot Corporation, Samsung Electronics, Neato Robotics,
LG Electronics) surgical robots (e.g., Intuitive Surgical, MAKO Surgical
Corporation), drones (e.g., Parrot SA, 3D Robotics, D-Jing Innovations Science
and Technology) and toys (e.g., Hasbro, WowWee Group Limited).  Examples of
successful commercial social robots are robotic pets developed in largest public
research organization and companies, such as Paro Therapeutic Robot (AIST), PLEO
(Innvo Labs established by Jetta Corporation), AIBO (Sony Corporation), Keepon
(BeatBots), and iCat (Philips).  Other examples can be found in the context of
smart home assistants; Mykie robot is a side project of BOSCH connected to an
IoT ecosystem. The robot is designed to support users while they cook providing
assistance for recipes and stock market prices. Mykie robot is activated and
controlled by Amazon’s Alexa, conversational virtual agents that nowadays
dominates the market of smart homes devices~\cite{purington2017alexa,
sciuto2018hey}.

Having social capabilities embedded in robotic systems provide an additional
emotional grip and more fluid interaction with humans, but is not enough for
justifying the needs of social robots over long periods~\cite{leite2013social,
baraka2019extended}. The human-robot interaction should be driven by clear
scopes, and the robot should be able to adapt to diverse and unstructured
environments. However, notwithstanding the improvements in computer vision and
machine learning, very few fully-developed intelligent autonomous systems
capable of learning from the real-world and successfully interact with humans
are currently available to consumers~\cite{dereshev2019longterm}. Thus, all
these companies are trying to adapt their business principles and products for
surviving against competitors breakthrough technologies and new customer
demands. The paradoxes encountered by the research and industry involve three
main actors: final users, organizations (companies) and researchers.

~\cite{tulli2019great}
~\cite{hoffman2019anki}

One example of a small, rugged robot designed for intensive use in school
environments is Cellulo~\footcite{ozgur2017cellulo}.

\begin{figure}[!htbp]
    \begin{minipage}[b]{.3\linewidth}
        \centering\includegraphics[height=4cm]{figs/cellulo.jpg}
        \subcaption{EPFL's Cellulo robot}\label{fig:cellulo}
    \end{minipage}%
    \hspace{0.5cm}
    \begin{minipage}[b]{.3\linewidth}
        \centering
        \includegraphics[height=4cm]{figs/tega.jpg}
        \subcaption{MediaLab's Tega robot}\label{fig:tega}
    \end{minipage}
    \hspace{0.5cm}
    \begin{minipage}[b]{.3\linewidth}
        \centering
        %\includegraphics[height=4cm]{figs/ono.png}
        \subcaption{OPSORO's Ono robot}\label{fig:ono}
    \end{minipage}
    \caption{Existing research-level companion robots}\label{fig:research-robots}
\end{figure}

\begin{figure}[!htbp]
    \begin{minipage}[b]{.3\linewidth}
        \centering\includegraphics[height=4cm]{figs/miro.jpg}
        \subcaption{Consequential's Miro robot}\label{fig:miro}
    \end{minipage}%
    \hspace{0.1cm}
    \begin{minipage}[b]{.3\linewidth}
        \centering
        \includegraphics[height=4cm]{figs/hatchnimals.jpg}
        \subcaption{SpinMaster's Hatchimals}\label{fig:hatchimals}
    \end{minipage}%
    \hspace{0.1cm}
    \begin{minipage}[b]{.3\linewidth}
        \centering
        \includegraphics[height=4cm]{figs/anki-vector.jpg}
        \subcaption{Anki's Vector}\label{fig:vector}
    \end{minipage}
    \caption{Existing commercial companion robots}\label{fig:commercial-robots}
\end{figure}


\subsubsection{Sustaining long-term interactions}

In his analyse of why many commercial projects around social robotics failed,
Hoffman cites the lack of long-term acceptance as one major
issue~\cite{hoffman2019anki}. He offers as key explanation ``the inability of
the robots to escape the single turn structure of an interaction'', which also
tighty connects to the issue of the repetitiveness of the robots' behaviours.

~\cite{dereshev2019longterm}

\subsubsection{Social robotics and developmental pathopsychology}


Robotics and autism~\cite{pennisi2016autism}

The false belief experiment that we have mentioned above, was proposed by
Baron-Cohen in the frame of his research on autistic spectrum disorders (he
shows that autistic children seem to actually lack a theory of mind and suggests
this as the primary cause of their social impairments), and Frith and Happé
further note in ~\cite{frith1994autism} that this specific deficit of autism has
led to a large amount of research which proved, in turn, highly beneficial to
the study of the development of theory of mind in general. They reference
in~\cite{frith1994autism} eight such tasks (Table~\ref{mentalizing-tasks}),
identified during the study of social cognition by autistic children. Each of
them is proposed in two versions: one does not require mentalizing, while the
other does require it.  One of these tasks, for example, required children to
distinguish emotions, namely happy/sad faces on one hand (\emph{situation-based}
emotion), and surprised faces on the other (\emph{belief-based}
emotion)~\cite{baron1993children}.  Another task, based on the
\emph{penny-hiding game}, contrasts the two conditions in terms of \emph{object
occlusion} vs.~\emph{information occlusion}~\cite{baron1992out} (we detail it
hereafter). These tasks prototypically illustrate social meta-cognition: one
need to represent and reflect on someone else representations (and not only
perceptions), and they are not addressed by today's research on social robots.

Experimental protocols in research on autistic spectrum disorders are often
striking by their apparent straightforwardness because of the careful choice of
interaction modalities: since autistic children frequently exhibit impairments
beyond social ones (such as motor or linguistic ones), the experiments must be
designed such that they require only basic cognitive skills beyond the social
abilities that are tested. The Sally and Anne task, for instance, requires the
observing child to be able to visually follow the marble, to remember the true
location of the marble, to understand simple questions (``Where will Sally look
for her marble?'' in Baron-Cohen's protocol~\cite{baron1985does}) and eventually
to give an answer, either verbally or with a gesture -- the two first points
being actually explicitly checked through questions: ``Where is the marble
really?'' (reality control question) and ``Where was the marble in the
beginning?'' (memory control question).

Likewise, current social robots have limited cognitive skills (no fast yet fine
motor skills, limited speech production and understanding, limited scene
segmentation and object recognition capabilities, etc.) and such tasks
that effectively test a single cognitive skill (in this case, mentalizing) in
near isolation are of high relevance for experimental social robotics.

\begin{table}[h]
    \centering
    \begin{tabular}{p{0.4\linewidth}p{0.5\linewidth}}
        \toprule
        No mentalizing required           & Mentalizing required          \\
        \midrule
        Ordering behavioural pictures     & Ordering mentalistic pictures~\cite{baron1986mechanical} \\
        Understanding see                 & Understanding know~\cite{perner1989exploration}            \\
        Protoimperative pointing          & Protodeclarative pointing~\cite{baron1989perceptual}     \\
        Sabotage                          & Deception~\cite{sodian1992deception}                     \\
        False photographs                 & False beliefs~\cite{leslie1992domain}                 \\
        Recognizing happiness and sadness & Recognizing surprise~\cite{baron1993children}          \\
        Object occlusion                  & Information occlusion~\cite{baron1992out}         \\
        Literal expression                & Metaphorical expression~\cite{happe1993communicative}       \\
        \bottomrule
    \end{tabular}
    \caption{\small Tasks requiring or not mentalizing to pass, listed by Frith and Happé in~\cite{frith1994autism}}
    \label{mentalizing-tasks}
\end{table}

Frith and Happé's list (Table~\ref{mentalizing-tasks}) is in that regard
especially interesting in that it mirrors pairs of task (ones which do not
require mentalizing with similar ones which do require mentalizing), thus
providing control tasks.  \emph{Object occlusion} vs.~\emph{Information
occlusion} is one example of a (pair of) task(s) which evidence
representation-level perspective taking through \emph{adaptive deception}:
during a simple game, the experimenter adapts its strategy
(deceptive/non-deceptive behaviour) to the representation skills of its child
opponent. The experimental setting is derived from the penny-hiding game
protocol originally proposed by Oswald and Ollendick~\cite{oswald1989role} and
replicated and extended by Baron-Cohen in~\cite{baron1992out}, who describes it
as a two-person game in which the subject is actively involved, either as a
guesser or as a hider. The hider hides the penny in one hand or the other, and
then invites a guess. The game is repeated several time before switching the
roles. Baron-Cohen proposes a specific index to rate the level of the players
based on the idea of \emph{information occlusion}: minimally, the hider must
ensure \emph{object occlusion} (the penny must not become visible to the
guesser), while good hiders, with representation-level perspective taking
skills, develop strategies (like random hand switching or deictic hints at the
wrong hand) to prevent the guesser to find the penny (\emph{information
occlusion}). One could imagine a similar protocol adapted to robotics: the robot
would play the role of the experimenter, adapting on-line its
behaviour to what it understands of the perspective taking capabilities of the
children, and would consequently require \emph{second-order},
\emph{representation-level} perspective taking from the robot.


\subsubsection{Conclusion: why long-term social interaction is beyond-state-of-art}



Some guidelines:

~\cite{tulli2019great}
~\cite{hoffman2019anki}

~\cite{tonkin2018design}


~\cite{baraka2019extended}









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{The long-term vision: robot-supported human-human interactions}


\begin{figure}
\centering
\includegraphics[width=0.9\linewidth]{figs/rHHI-1}
\caption{A child, just arriving in a new school, tries to integrate with other
    children -- in such a situation, should the robot decide to help to break the ice? What
    are the socio-cognitive peceptual and behavioural capabilities required to
    make that decision?}
\label{fig:rHHI}
\end{figure}


\project is a project that aims at helping to build strong human
relationships with the help of technology.

The core idea of the project is to build small companion robots whose
aim is to facilitate human-human interactions. We want to develop these
robots with a particular application in mind: supporting the social and
cultural integration of vulnerable children in a foreign country, and in
particular, migrant children who might lack the otherwise needed support
(shared culture; already well integrated relatives) for a successful
integration.

\subsubsection{Key scientific research questions}

\begin{enumerate}
\item research the role of social influence to scaffold positive human-human
    interactions (including its ethical ramifications) in the context of migrant
    integration;
\item explore how a small robot companion can be endowed with social
    competencies to positively influence human interactions; design and
    implement the corresponding artificial social behaviours;
\item design and build a small rugged and autonomous companion robot that
    realise this goal for child-child interactions; test the technology in
    school with actual migrant children.
\end{enumerate}


\subsection{Three pillars}

The \project project will, for the first time, take a broad, holistic approach
to these questions, and as such, is build around three pilars:

\begin{enumerate}
    \item AI architecture for decision making
    \item socio-psychological design of the interaction, from 1-to-1
        interaction, to group interaction, to interaction with the eco-system
    \item built-in privacy and ethics
\end{enumerate}

The difficulty comes from the fact that these pilars do not represent independent
research questions: they are deeply woven together, in intricate ways.

Taken independently, these three pilars are already intrinsically complex.

The first pilar, AI Architecture for Decision Making, for instance:
state-of-the-art decision making processes for social robots are build from
multiple algorithmic layers: classical machine learning (like classifiers, or
SLAM) for low-level sensori-motor processing; probabilistic data processing (for
e.g. localisation or point cloud processing); deep neural networks for object
recognition, scene understanding, speech and dialogue processing; classical
symbolic reasoning (for instance, for task planning); high-level knowledge
representation and reasoning (semantic webs); hybrid spatial/symbolic reasoning
for motion planning; etc.

Importantly, these layers are not typically combined in a simple, linear
fashion: modern control architectures are distributed, loosely coupled, and
design so that information flows in multiple directions, at different levels of
abstraction, in order to create complex feedback loops, and achieve systems that are
both goal-directed \emph{and} reactive (event-driven).


The real complexity of such AI architectures is however striking when one tries
to answer \emph{how} such an architecture can enable the realisation of the
desired socio-cognitive functions, i.e. our second pilar. Social behaviours and
social situations are highly dynamic, loosely structured, underspecified (no
pre-made rulebook exists), uncertain.

Our best answer is for the robot to learn to become social: a human expert teaches
the robot how to socially behave, in-situ, with the robot embedded in the
interaction. I have conducted two recent and high-profile experiments in this
direction







\project is also a highly technical project, who aims at significantly
pushing the state-of-art in autonomous social robotics. Indeed, in \project, we will
\textbf{implement the AI required for robots to effectively support
human social interactions}.  In that sense, this research is also
ground-breaking in regards to its technical objectives. In \project, robots will
be able to understand complex social dynamics, and generate appropriate social
responses, in a fully autonomous way.  Extending the current line of research of
the PI, we will identify, implement, and integrate the a broad range of cognitive functions into a
principled, socially-driven, and trustworthy socio-cognitive architecture for
robots. This is the second major expected scientific outcome of \project.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The case for robot-supported human-human interactions}

This change of paradigm (rHHI) will have
far reaching impact on the place that we collectively assign to social robots in
the society; it will help to structure the public debate by providing the
framing to look at human-robot interactions in term of their net social utility.
As such, \textbf{the first outcome of \project will be researching, defining and implementing the conceptual
framework that we need to ensure trustworthy and socially responsible robots in
our societies}.


Indeed, in a time where robots are on the verge of becoming pervasive in our
daily human environment, can we ensure 'by design' that robots are to become
powerful tools to support more and better, positive interactions \emph{between
the people themselves}, and build a stronger, more cohesive society? Beyond
Human-Computer Interaction (HCI) and Human-Robot Interaction (HRI), \project is
a forward-looking, ground-breaking project that aims at establishing
\textbf{Robot-supported Human-Human Interactions (r-HHI)} as the next step
toward a Responsible AI: \textbf{the scientific investigation of how social
robots can create, shape and support strong, sustained, positive
\emph{human-human} relationships}.

As a researcher who has been working for the last 12 years in the field of
human-robot interaction (and child-robot interaction in particular), I have been
a direct witness -- by being one of the architects -- of the crossing of a
critical milestone: the emergence of \textbf{long-term social interactions}
between robots and humans. Over the last two years in particular, we observe an
explosion of the number of studies involving social robots, deployed in
real-world settings (schools, care centres) over relatively long periods of time
(up to 2 or 3 months at a time)~\cite{kunze2018artificial,leite2013social}. Even
though these robots are rarely fully autonomous, they do already show high
levels of autonomy~\cite{senft2019teaching}, with full autonomy in
sight~\cite{hawes2017strands}.

While many see these developments positively,
others express skepticism or worry that this technology might lead to a
dehumanisation of our society. Indeed, if artificial agents are to engage into
social interactions with us, and enter what many view as a form of a 'private
domain', reserved to humans, pressing ethical questions need to be answered:

\begin{itemize}
    \item how to ensure that social robots are not used to simply replace the human
        workforce to cut costs?
    \item can we provide guarantees that the use of social robots will always be
        ethically motivated?
    \item further on, can we implement some ethical safeguarding built-in
        the system (like an ethical \emph{black-box}~\cite{winfield2017case})?
    \item what about privacy? how to trust robots in our home or school or
        hospital not to eavesdrop on our private lives, and, in the worst
        case, not be used \emph{against} us?
\end{itemize}

These questions are not only legitimate, but also pressing. The recent rise of
personal assistants like Amazon Alexa or Google Home, with the major privacy
concerns that accompanies their deployments in people home, shows that letting
the industry set the agenda on these questions is not entirely wise -- and
robots can potentially be much more intrusive than non-mobile smart speakers.
The EU is positioning itself at the forefront of those questions. The recent
release of operational \emph{Ethics Guidelines for Trustworthy AI} by the EU
High-level Expert Group on Artificial Intelligence~\cite{eu2019ethics} is a
strong sign of this commitment.  However, personal \& social robots raise a new
class of questions regarding what ethical and trustworthy systems might look
like, and while the principles of responsible design are somewhat
established~\cite{stahl2016ethics, bsi2016robots}, the reality of
robot-influenced social interactions is not understood yet, if only because the
technology required to experience such interactions is only slowly maturing. 

Social robots have indeed two properties that stand out, and distinguish them from
these smart speakers, while making them significantly harder to conceptually frame.
First, they are fully embodied, and they physically interact with their
environment, from moving around, to picking up objects, to looking at you;
second, willingly or not, they are ascribed \emph{agency} by people. This second
difference has far-reaching consequences, from affective bonding to over-trust,
to over-disclosure of personal, possibly sensitive,
informations~\cite{martelaro2016tell,shiomi2017robot}.


Due to the complex interplay between the socio-psychological determinants of the
interactions, the technical implementation, and the multiple ethical mechanisms
that have to be built-in the system, it is difficult to build one coherent and
consistent perspective on the whole question. Indeed, the conceptual,
intellectual framework emcompassing both the internal cognitive mechanisms
required by socially intelligent robots, as well as their role and impact in the
society, only exists in fragmented, disconnected pieces~\cite{citeneeded}.


The lack of such a proper conceptual framing is a critical issue: for robots to
have a positive impact on the society, with strict ethical and privacy-related
safeguarding, it is urgent that the wider academic and intellectual communities,
beyond technologists, embrace this question and build up the debate on the
acceptability of robots in our society. And \textbf{this also calls for a major
re-thinking of the traditional paradigm of Human-Robot Interaction}.
Tradionally, individual cognitive functions (like natural language processing,
emotion recognition, proxemics-aware navigation) are implemented in robots, with
the ill-supported assumption that 'the more available functions, the more
socially-capable the robot'. This assumption is questionable, and, at any rate,
does not address the 'why': why does the robot decides to do what it does? What
drives the behaviour of the robot? The case for \emph{teleological} (ie
goal-driven) robotic architectures has been made in the
past~\cite{wrede2012towards}, but only effectively realised for relatively
simple cognitive systems (like curiosity-driven robot
animals~\cite{oudeyer2005playground} or motor babbling in infant-like
robots~\cite{forestier2017unified}). However, socially-driven robots,
participating in complex interactions with humans, have been barely
investigated. We need to create a new social purpose, a new social teleology to
drive the development of social robots.  Indeed, \textbf{being socially-driven
to do 'good' is essential in ensuring trustworthy, socially responsible robots}.
Nutrured by decades of research in understanding human social cognition and its
social motivations, one of the key purpose of \project is to \textbf{research
and build a principled and socially-driven cognitive architecture} for
tomorrow's social robots. This socially-driven, teleological architecture,
intrinsically designed to \textbf{support stronger, positive human-human
interactions} is what underpins the concept of \emph{robot-supported human-human
interactions}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

While the original poster picture of a
'multi-function-household-robot-that-does-the-dishes' has yet to materialise,
social robots are certainly in the process of establishing themselves as
important agents in a variety of other situations, for their intrinsic
\emph{social} features: source of comfort, tutors, entertainers. The robot as a
\emph{social agent} is a reality, within laboratories at least.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{[Blocks of text for reuse]}


im robots can have a lasting, positive
impact on the society, by fostering social interactions amongst humans
themselves. It defines the new concept of \textbf{robot-supported human-human
interactions} as a general framework to study and implement socially-aware
intelligent robots, able to understand social interaction, and, when
appropriate, act to create and support positive interactions between people.


Grounded in both the psycho-social literature of human cognition, and the latest
technological advances in human-robot interaction, the project delivers
major conceptual, technical and experimental contributions to the field of AI, 
with a particular focus on ethics \& safeguarding mechanisms, in order to build 'by 
design' a trustworthy AI system.

\project delivers this programme by building on a range of multidisciplinary
methods, including sociological investigation, novel interactive machine
learning techniques, and a pervasive approach to co-design that puts the
end-user needs at the centre of the design process. It paves the way for a
better understanding of the societal challenges raised by the rapid development
of AI and robotics, and, critically, opens a \textbf{unique window into what
positive role social robots could play in our future societies}.




\project aims at building unique European capacity to assert leadership in this
domain, and, beyond the specific deliverables of this 5-years project,
establishing the PI as a world-leader in goal-driven, socially-responsible
robotics.



While ambitious, the project's feasibility is also ensured through a strong
combination of cross-disciplinary expertise and support from the host's unique
research infrastructure. Scaffolded by a participatory and iterative research
methodology, and the PI experience in managing teams and complex projects
through his recognised leadership, \project is set to deliver.

To mirror the cross-disciplinary team, the research methodology developped for
\project will rely heavily on \emph{mutual shaping}, through co-design and
participatory design, and multi-disciplinary development sprints.

Software-wise, PI Lemaignan is a leading developer of software for intelligent
robots, with numerous contributions to the major software platforms used in
robotic (including major contributions to ROS, OpenCV, authoring of widely-used
pieces of software like the ROS-naoqi bridge used by hundred of researchers
using the Nao and Pepper robots or the MORSE robotic simulator). He has been
teaching various modules related to software development for robotics, and the
breadth and depth of his knowledge of robotic software development is well
established. He will be leading the software development, and, as part of the
fellowship, he will also seek for additional expert knowledge on (1) interaction
design and (2) low-level micro-programming to complete the set of required
programming needs.






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Companion robots}

\section{Impact}\label{impact}

\subsection{Impact on society and technology: building an inclusive society}


\project will deliver new and fundamental knowledge to the fields of robotics,
computer science and psychology, in addition to improving trans-disciplinary
understanding between these disciplines. More specifically, the project will
contribute to a better understanding of the following research areas:
human-robot interaction; human-machine interaction; human error making and
handling in assembly tasks; theory of mind and its transfer to cognitive robots;
natural language processing; explainability and language generation; machine
vision and human activity detection; and action planning under uncertainty. New
interaction paradigms will be developed for handling error situations where
machines interact with non-expert humans. The integration of perception and
sensing into cognitive robot architectures will be critically reviewed and
extended. Novel, empirically informed methods to transfer findings from
human-human interaction studies to human-machine interactions will be developed.
Furthermore, the national and international psychology, robotics and computer
science research communities will benefit from the project results. We will
publish \project results in interdisciplinary and discipline-specific journals
and conferences, and organise \project-themed workshops. Please refer to Section
\ref{sec:impact} about our concrete action plan to maximise impact. 




Academically, the \project project represents a timely combination of
very recent advances in supervised machine learning for social robot
behaviour with a creative and interdisciplinary approach to the design
and automation of social robot behaviour. We therefore expect to publish
results in high-class scientific journals and conferences.

The dataset of social behaviours and social signals we will create and
distribute represents a one-in-a-kind resource for the human robot
interaction community, and the human data collection will be
transferable to research in other domains such as human-computer
interaction.

As \project will be deployed in a living lab environment, there is
significant scope for public outreach/engagement and media coverage,
which we will work with the BRL's media manager to maximise.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{B2.b Methodology}\label{research-methodology}

\eu{Describe the proposed methodology in detail including any key intermediate
goals. Explain and justify the methodology in relation to the state of the art,
and particularly novel or unconventional aspects addressing the
'high-risk/high-gain' balance. Highlight any intermediate stages where results
may require adjustments to the project planning. In case you ask that team
members are engaged by another host institution their participation has to be
fully justified by the scientific added value they bring to the project.}

%\subsection{Gantt chart}\label{gantt-chart}

\section{Workpackages overview and interrelations}\label{workpackage-interrelations}

\begin{figure}[h!]
\centering
\includegraphics[width=\linewidth]{figs/archi}
\caption{Overview of the workpackages and tasks, and tasks inter-relations.}
\label{fig:archi}
\end{figure}

Figure~\ref{fig:archi} gives an overview of the project workpackages, and their
interrelations. Fieldwork plays a central role in the project, and appears in
the centre of the figure. The first important field deployment is a one-year
experiment, taking place at the Bristol Science museum (T1.1). This
'public-in-the-loop' experiment is analysed and lead to the definition of core
interaction principles (T1.2). These are in turn translated into algorithmic
models, guiding the social teleology of the cognitive architecture (T3.1).

This first experiment is immediately followed by two other long-term
experimental deployments: a one-year deployment in one of Bristol's Special
Education Need (SEN) school (T5.1), followed by a one-year deployment at
Bristol's Children hospital (T5.2). These two additional experiments are both
inputs for WP2 and WP4, and demonstrator for the robot socio-cognitive
architecture (WP3).

Specifically, workpackage WP2 research, develop, and integrate all the components
pertaining to the assessment of the spatio-temporal and social environment of
the robot. Reference interaction situations and the data required to support
this workpackage is directly drawn from the experimental fieldwork that will
take place at the same time in WP1 and WP5. The perceptual capabilities
delivered by WP2 are continuously integrated into the robot's cognitive
architecture (T3.3), iteratively improving the socio-cognitive performances of
the robot.

Workpackage WP4 looks into behaviour generation using machine learning (T4.2)
and non-verbal affective modalities (T4.3). T4.2 is data-intensive, and will use
datasets acquired during the field deployments (T1.1, T5.1, T5.2), as well as
lab-recorded dataset of social interactions. Similar to WP2, the capabilities
built in WP4 are integrated in the robot architecture in T3.3.

In addition to the integration of WP2 and WP4 capabilities, WP3 is also
researching and developing the socio-cognitive drives of the architecture. They
come both from T1.2 (as previously mentioned), and
human-in-the-loop/public-in-the-loop machine learning (T3.2). T3.2, in
particular, is tighly connected to the experimental fieldwork, where the
learning-from-end-users take place.

\subsection{Milestones}\label{milestones}

\begin{itemize}

\item   week-long tests with local children in local schools
\item   field deployment with one child in one school
\end{itemize}

\begin{table}[!htbp]
\caption{List of milestones}
\centering
\begin{tabular}{@{}lllll@{}}
\toprule
\textbf{Milestone number} & \textbf{Milestone name} & \textbf{Related work package(s)} & \textbf{Estimated date} & \textbf{Means of verification} \\ \midrule
                          &                         &                                  &                         &                                \\
                          &                         &                                  &                         &                                \\
                          &                         &                                  &                         &                                \\
                          &                         &                                  &                         &                                \\ \bottomrule
\end{tabular}
\end{table}


\subsection{Deliverables overview}\label{deliverables-overview}

\begin{table}[!htbp]
\caption{List of deliverables}
\begin{tabular}{@{}lllllll@{}}
\toprule
\textbf{Deliverable} & \textbf{Deliverable name} & \textbf{Work package No} & \textbf{Lead participant short name} & \textbf{Type} & \textbf{Dissemination level} & \textbf{Delivery date} \\ \midrule
D1.1                 &                           &                          &                                      &               &                              &                        \\
D1.2                 &                           &                          &                                      &               &                              &                        \\
D2.1                 &                           &                          &                                      &               &                              &                        \\
...                  &                           &                          &                                      &               &                              &                        \\ \bottomrule
\end{tabular}
\end{table}

Type:

\begin{itemize}

\item   R: Document, report (excluding the periodic and final reports)
\item   DEM: Demonstrator, pilot, prototype, plan designs
\item   DEC: Websites, patents filing, press \& media actions, videos, etc.
\item   OTHER: Software, technical diagram, etc.
\end{itemize}

Dissemination level:

\begin{itemize}

\item   PU = Public, fully open, e.g.~web
\item   CO = Confidential, restricted under conditions set out in Model Grant
  Agreement
\item   CI = Classified, information as referred to in Commission Decision
  2001/844/EC.
\end{itemize}





\begin{landscape}
\include{gantt}
\end{landscape}

\newpage


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{WP1: \textbf{\wpOne}}

\noindent\framebox[\linewidth]{Duration: \textbf{Y1-Y3}; one senior post-doc
with background in sociology of technology}

The basic ambition of \project is to create a conceptual framework around social
robots in the human society, by re-framing the tradionally accepted idea of
\emph{human-robot interaction} into the human-centered idea of
\emph{robot-supported human-human interactions} (r-HHI): the robot is considered
from the perspective of how it can \emph{support} humans, and in particular,
support stronger, positive interactions between humans.

\textbf{T1.1 -- Conceptual framing of r-HHI} The first task in WP1 is to research and
define such a framework that will provide the (currently missing) conceptual
frame around questions like: what role for social robots? where to set the
boundaries of artificial social interactions? what does 'ethical-by-design',
'responsible-by-design' might mean in the context of social human-robot
interactions? 

In order to anchor T1.1 into the reality and complexity of human social
interactions, and to also involve the civil society in this framing process, the
task will embed \project into the 'City lab' experiment, conducted by Bristol's
science museum WeTheCurious. WeTheCurious is leading the push for a new form of
public engagement, call 'City Lab', that sees the visitors engaging in the
actual production of science. We will integrate \project in the City Lab to
co-design and co-produce robot-supported social interactions with the general
public. For an initial period of one year (Y2-Y3), one \project robot will be
permanently based at the museum.  Participants (children and adults) will be
guided, with the help of museum staff and a dedicated interface, into
teleoperating the robots to make them good 'social helpers'. This will generate
the quantitative and qualitative data to inform questions like 'what role for
the robot?', 'when to intervene?', 'what are the effective and acceptable social
influence techniques?'. It will also be a unique example of large-scale
participatory design with future end-users of social robots.

\textbf{Specific resources} I have an on-going collaboration with WeTheCurious,
and preliminary meetings were held to discuss specific requirements for the
\project project. The museum is committed to the project, and will include
\project in its official programme of activities.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% WeTheCurious
% 
% - one robot completelty controlled by children, one by adults
% 
% what to learn?
% 
% - when to approach? when to prompt? [example of the salesman/museum facilitator]
% - when is the right time to help/intervene or not? 'child being told off by
% parents -> not the right time!'
% - group interactions -> when to intervene? what about peer-pressure? eg what if
% I tell off one child in front of another?
% - break the barrier for participation. Japanese Journal paper -> facilitating students questions
% - impact on moral norms? what behaviours is acceptable?
% - what role for the robot? another mediator? a peer?
% - what can we do with that 'alien creature'
% 
% - robot taking one child to talk to the museum mediators ("I, robot, am  shy!
% would you come with me?")
% 
% - learning how to adjust behaviour based on personality
% - 'why do I behave like that with that person, and like this with that other
% person?'
% 
% - reinforcement learning instead of human-in-the-loop -> what reinforcement
% signal? engagement
% 
% - the robot that 'take sides': take side against the adults? -> bending in its
% role?
% 
% 
% - social embarassment
% - space for pretence: the robot can adopt an 'artificial role' as long as it is
% possible (accpetable/...) to pretend the robot is



\textbf{T1.2 -- Determinants and principles of robot-supported social
interactions} The conceptual framework identified in T1.1 is translated
into a set of \emph{interaction design principles}, \emph{determinants} and
\emph{parameters} that will together form a set of requirements and objectives
for the socio-cognitive capabilities and architecture developed in WP2 and WP3.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{WP2: \textbf{\wpTwo}}

\noindent\framebox[\linewidth]{Duration: \textbf{Y1-Y4}; one post-doc in social
signal processing/machine learning/cognitive modelling}

In WP2, the project addresses the key scientific and technical pre-requisites to
effectively deliver WP3's architecture:  the scientific understanding and
formalisation of the \emph{social fabric} in which the robot is embedded, in its
full complexity: spatial characteristics (proxemics; group dynamics; complex,
dynamic attentional mechanisms); psycho-social determinants (social roles and
hierarchies; social groups; mental modelling; anthropomorphic ascriptions);
temporal characteristics (effects of novelty; dynamics of anthropomorphism and
mental ascriptions; group dynamics).  While several of these capabilities have
been previously investigated is isolation~\cite{lemaignan2014dynamics,
flook2019impact,lemaignan2015youre, fink2014which, ros2010which,
warnier2012when, lemaignan2015mutual, dillenbourg2016symmetry,
winkle2019effective}, this WP will deliver the first complete and integrated
model of artificial cognition that account for social interactions in their full
extend, significantly extending the state-of-art~\cite{lemaignan2017artificial,
baxter2016cognitive}.


\textbf{T2.1 -- Hybrid situation assessment and knowledge representation}
Knowledge representation and grounding is a fundamental building block for
cognitive architectures~\cite{lemaignan2017artificial,beetz2010cram}. This task
builds on existing state-of-art in knowledge representation and situation
assessment (eg~\cite{citeneeded}) and creates a coherent system of
representations for the cognitive architecture that extends the \sc{underworlds}
spatio-temporal representation tool developped by the
PI~\cite{lemaignan2018underworlds,sallami2019simulation} with knowledge
representation capabilities, using both established symbolic techniques (like
ontologies and first-order logic~\cite{lemaignan2010oro, tenorth2009knowrob}),
and hybrid symbolic/sub-symbolic modelling (using Jaeger's
conceptors~\cite{jaeger2014controlling}) as a new route to overcome the symbolic
grounding problem~\cite{harnad1990symbol}.

\textbf{T2.2 -- Social dynamics} This task focuses on the processing and
modelling of social signals, extending existing techniques, both model-based
(eg~\cite{lemaignan2016realtime,others}) and machine-learning based
(eg~\cite{chetouani,others}) This task goes beyond the state-of-the-art by
looking specifically at resolving highly dynamical signals (like gaze saccades
and micro facial expressions). While playing an fundamental role in social
interactions~\cite{citeneeded}, they are currently not investigated in social
robotics -- even though the technology (high speed cameras and embedded GPUs) to
achieve real-time classification of such cues is available.

\textbf{T2.3 -- Interaction and group dynamics} Building on T2.2, T2.3
investigates the automatic understanding and modelling of group-level social
interactions, like inter-personal affordances~\cite{pandey2013affordance}. It
includes spatial determinants (proxemics; group-level attention tracking);
psycho-social determinants (social roles and hierarchies; social groups) and
dynamics (effects of novelty; dynamics of anthropomorphism and mental
ascriptions; group dynamics). 


\textbf{T2.4 -- Social situation assessment} The integration of the social cues
from T2.2 and T2.3 results in a socio-cognitive model of the social environment
of the robot that we term \emph{social situation assessment}.  It effectively
extends the representation capabilities of T2.1 to the social sphere, and covers
the development of a complete social assessment pipeline, from social signal
perception (like automatic attention tracking, face recognition, sound
localisation, etc.) to higher-level socio-cognitive constructs, including group
dynamics and theory of mind (as I previously framed
in~\cite{lemaignan2015mutual, dillenbourg2016symmetry}). A focused experimental
programme accompanies T2.4, to demonstrate (in relative isolation) the resulting
socio-cognitive capabilties. In particular, the protocols identified by Frith
and Happé~\cite{frith1994autism} to investigate theory of mind with autistic
children offers an excellent experimental framework for social
robotics~\cite{lemaignan2015mutual} and will be employed.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{WP3: \textbf{\wpThree}}

\noindent\framebox[\linewidth]{Duration: \textbf{Y1-Y4}; one senior post-doc in
cognitive robotics; one PhD student}


This part of the programme is the technical core of the project: we will create a
novel socio-cognitive architecture for the robots, bringing together advanced
perception of the human social dynamics, intrinsic motivation to support human
interactions, and human-in-the-loop machine learning to create transparent,
trustworthy action policies. This WP is high-risk/high-gain, as no such combined
approach has been successfully implemented and deployed in real-world, complex
social situations. I mitigate the risk by ensuring cognitive functions are
decoupled from each other where sensible, and in particular, by ensuring that
the robot actions are generated independently through both an intrinsic
motivation mechanism, and a human-taught machine learning action policy, hence
creating a level of cognitive redundancy (with the corresponding arbitration
mechanisms in place where necessary).


\paragraph{Disembodied Cognitive Architectures}

Reviews: \cite{chong2007integrated}, \cite{vernon2007survey} --
extended in \cite{kingdon2008review}, \cite{duch2008cognitive},
\cite{langley2009cognitive}, \cite{taatgen2010past},
\cite{thorisson2012cognitive}.


\paragraph{Social Robotics Architectures}\label{sec:robots}


\emph{Society of Mind}-inspired paradigm underpinning practical architectures

Mostly functional architectures: integration models rather than cognitive
architectures per se.

\begin{figure}
    \centering

    \resizebox{\linewidth}{!}{%

        \tikzset{subpart/.style={draw, font=\scriptsize, fill opacity=0.5, text opacity=1, fill=white!50}}
    \begin{tikzpicture}[
            >=latex,
        every edge/.style={draw, very thick},
        skill/.style={draw, align=center, inner sep=5pt, fill=black!20},
        stmt/.style={align=center, font=\bf},
        label/.style={midway, align=center, font=\scriptsize, fill=white}]

        %%% Knowledge base
        \node at (0,0)[skill] (kb) {\bf Memory/Knowledge Base(s)\\ \footnotesize for instance, a symbolic blackboard};

        %%% Symbolic task planner
        \node at (-6, 2.5)[skill] (taskplanner) {{\bf Symbolic Task planner}\\ \footnotesize possibly human-aware};

        %%% Communication
        \node at (-6, -3) [skill] (communication) {{\bf Multi-modal communication}\\NLP, back-channel,...};

        %%% Abstract world model
        \node at (4,-3.5)[skill] (abstractenv) {%
            \begin{tikzpicture}
                \node at (0,0) (geom) {\bf Amodal Model of the Environment};
                \node [subpart, below=0.2 of geom.south west, anchor=north] (world-update) {Sensors fusion};
                \node [subpart, right=0.2 of world-update] (geom-model) {Situation assessment};
                \node [subpart, right=0.2 of geom-model] (fact-prod) {Temporal reasoning};
            \end{tikzpicture}
        };

        %%% Motion planner
        \node at (8.5,0)[skill] (motionplanner) {\bf Motion and manipulation \\ \bf planning};

        %%% Supervision
        \node at (4,4.5)[skill] (supervisor) {%
            \begin{tikzpicture}
                \node at (0,0) (exec) {\bf Supervisor};
                \node [subpart, below=0.2 of exec.south west, anchor=north west] (plans) {Goal \& Plans \\ management};
                \node [subpart, right=0.2 of plans] (sit-asses) {Context management};
                \node [subpart, right=0.2 of sit-asses] {Action instantiation, \\ execution and monitoring};
            \end{tikzpicture}
        };


        %%% LOWLEVEL
        \node [skill, below=0.7 of abstractenv,minimum width=6cm,minimum height=1.5cm,fill=black!30] (lowlevel) {\bf Sensorimotor layer};

        %%% Separation between deliberative layer and sensori-motor layer
        \draw[dotted, thick] (-8,-5) -- (12, -5);

        %%% Relations between components
        \path (supervisor.340) edge [<->, bend left] node[label] {motion plan \\ requests} (motionplanner);
        \path (supervisor.west) edge [<->, bend right] node[label] {plans} (taskplanner);
        \path (taskplanner) edge [<->, bend right] node[label] (domain) {world model and \\ agents beliefs} (kb.170);
        \path (communication) edge [<->, bend left] node[label] (nlp) {communication \\ grounding} (kb.190);
        \path (abstractenv.100) edge [->, bend right] node[label] (symfact) {abstract world\\ description} (kb);
        \path (abstractenv.5) edge [->, bend right] node[label] {geometric\\model} (motionplanner);
        \path (supervisor) edge [<->, bend left] node[label] (evts) {events, \\ world model and \\ agents beliefs} (kb);
        \path (lowlevel) edge [->] (abstractenv);
        \path (lowlevel.east) edge [<-, bend right=80, looseness=1.2] node[label] {atomic\\actions} (supervisor.east);

    \end{tikzpicture}
    }
    \caption{High-level representation of a typical \emph{practical} software
    architecture found in social human-robot interaction (based
    on~\cite{lemaignan2016artificial}). Arrows represent interfaces
    between independent software modules. They evidence the engineering
    perspective often favoured in robotic: cognition is seen as the result of a
    collection of individual cognitive skills, each implemented as an
    independent software component.}

    \label{fig:robot-archi}
\end{figure}

ACT-R/E~\cite{trafton2013act}, HAMMER~\cite{demiris2006hierarchical}, PEIS
Ecology~\cite{saffiotti2005peis,daoutis2012cooperative},
CRAM/KnowRob~\cite{beetz2010cram, tenorth2009knowrob},
KeJia~\cite{chen2010developing}


\paragraph{To be sorted...}

To check -- they may or may not be all relevant to sHRI.

Architectures that...

\begin{itemize}
    \item ...reason about other agent mental state
    \begin{itemize}
        \item Scone~\cite{fahlman2011using}
        \item Polyscheme~\cite{bello2011shared}
    \end{itemize}
    \item ...'represent knowledge'
        \begin{itemize}
            \item \cite{zhang2014towards}
        \end{itemize}
    \item ...cover the 'whole interaction stack'
        \begin{itemize}
            \item POETICON++ arch~\cite{antunes2016from} 
        \end{itemize}
\end{itemize}

\subsection{T3.2}

The current state-of-the-art is limited in this respect: sub-symbolic
approaches focus on identifying low-level social signals (for instance,
gazing, gesture recognition) while symbolic approaches have focused on
classical symbolic problems like language understanding. Some attempts
have been made to bridge both (for instance, multi-modal dialogue
\cite{lemaignan2011grounding, lemaignan2017artificial}), but no work to
date has been able to interpret the social dynamics themselves (for
instance, \emph{are the partners getting along well?}, \emph{Is there
any frustration?}, \emph{Are they collaborating or not?}, \emph{Are they
engaged in their joint task?}). This is likely due to the complexity of
the problem, mixing sub-symbolic and symbolic reasoning in intricate
ways. Understanding social dynamics in naturalistic conditions entails
identifying a complex net of overlapping social cues, at multiple time
scales. Reasoning about these ever-changing, sometimes contradictory,
dynamics, is an essential skill for an artificial socio-cognitive
system, yet an essentially open research question.

I have recently introduced a dataset of social
interaction~\cite{lemaignan2018pinsoro} that enables for the first time a
quantitative, data-driven investigation of social dynamics. Promising initial
results led me to uncover three latent constructs that underpin social
interactions~\cite{bartlett2019what}. This dataset and the related on-going
scientific investigations will form the starting point of my approach to this
challenge.







\textbf{T3.1 -- A social teleology for robots}
The case for \emph{teleological} (ie goal-driven) robotic architectures has been
made in the past~\cite{wrede2012towards}, but only effectively realised for
relatively simple cognitive systems (like curiosity-driven robot
animals~\cite{oudeyer2005playground} or motor babbling in infant-like
robots~\cite{forestier2017unified}). Socially-driven robots, participating in
complex interactions with humans, have been barely investigated. This task
covers the overall design of the architecture.


\textbf{T3.2 -- Learning from humans to achieve 'by-design' responsible \&
trustworthy AI} Building on my recent, promising results on human-in-the-loop
social learning~\cite{senft2017supervised,senft2019teaching,winkle2020couch}, this task
implements the learning mechanics (including the critical aspect of the
interface with the human teacher) to allow human participants to progressively
teach the robot a social policy to become a good social helper.

In addition, this task researches how human-in-the-loop machine learning enables a more
trustworthy AI system, by involving the end-users in the creation of the robot
behaviours, guaranteeing a level of behavioural transparency for the end-users.

\textbf{T3.3 -- Integrating a socially-driven architecture for long-term interaction} The
socio-cognitive architecture of \project robots builds from the principles (the
`why's?') identified in T1.3, and relies on a combination of socially-driven
intrinsic motivation (a \emph{social teleology}, T3.1), and human-in-the-loop machine
learning (T3.2) to progressively learn an social policy enabling long-term
autonomy. This task focuses on `bringing the pieces together' in a principled
manner.

We will specifically look at the requirement for \emph{long term} autonomy: Over
the last two years, we have observed a significant increase of studies
involving social robots, deployed in real-world settings (schools, care centres)
over relatively long periods of time (up to 2 or 3 months at a
time)~\cite{kunze2018artificial,leite2013social}, with some promissing results
in well defined situations, with pre-defined tasks (for example, learning
tasks~\cite{senft2019teaching}, or 'butler' in a social care
facility~\cite{hawes2017strands}). More generic (long-term) social autonomy
however requires additional, beyond-state-of-art research to (1) add a
\emph{social motivation} mechanism able to drive the robot's intentions over
time. This is specifically investigated in T3.1 and T3.2 above; (2) a level of
cognitive redundancy to ensure reliable perception and behaviour generation
(addressed by this task, with a dependency on the cognitive functions developped
in WP2 and WP4).

Additionally, a critical aspect of task T3.3 is to develop the arbitration
mechanism that combines the robot's social teleology (T3.1) with the human-taught
action policy (T3.2). This arbitration mechanism will build on research on
reinforcement learning for experience transfer~\cite{madden2004transfer} that
enables the re-assessement of a policy (here, our intrisic motivation) based on
previous experience (here, the human-taught policy).



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{WP4: \textbf{\wpFour}} 

\noindent\framebox[\linewidth]{Duration: \textbf{Y2-Y5}; one post-doc in HRI/machine learning/learning from
demonstration}

Mirroring WP2's focus on understanding the social interactions, WP4 addresses the
question of social behaviour \emph{generation}: how to create natural
behaviours, engaging over a sustained period of time (eg not simply picking
scripted behaviours from a library, that are rapidly perceived as repetitive).

Using cloud-based speech recognition, the robots will be able to understand and
record the textual transcription of the what the end-users say (in WP5, mostly
children). The robots themselves are however purposefully designed \emph{not} to
speak, using instead non-verbal communication mechanisms (non-verbal utterances
using sounds, gaze, joint attention, expressive motions, etc). This is a
critical interaction design choice, that ensures we can more effectively manage
what cognitive capabilities are ascribed to the robot by the users (expectation
management).  \project seeks however to significantly push forward the
state-of-art of behaviour generation for robots, both in term of technique to
generate the behaviours, and in term of the nature of the non-verbal behaviours.


\subsection{T4.1 -- Behavioural baseline}

T4.1 establishes a baseline for behaviour
generation, by surveying and implementing the current state of the art. In
addition to traditional approaches like behaviour libraries, this will cover
techniques like curiosity-driven behaviours~\cite{oudeyer2005playground},
Learning from Demonstration~\cite{billard2008robot, argall2009survey},
human-in-the-loop action policy learning~\cite{senft2016sparc,
senft2019teaching}. This baseline will enable early in-situ experimental
deployments (WP5), while also provide a comparison point for T4.2.

\subsection{T4.2: Machine learning for continuous motion generation}

\project aims
at significantly advancing the state of the art in this regard, by combining two
existing techniques: (1) data-driven, continuous approach to behaviour
generation inspired by Learning from Demonstration; (2) interactive machine
learning in high-dimensional input/output spaces~\cite{senft2020woz}, where I
have shown with my students promising results for generating complex social
behaviours~\cite{senft2019teaching, winkle2020couch} that fully involve the
end-users~\cite{winkle2018social}.  By combining the two, I target
a breakthrough in robots' social behaviours generation: the generation of
non-repetitive, socially congruent and transparent social behaviours (including
gestures and gazes).


\begin{wrapfigure}{l}{7cm}
    \centering
    \includegraphics[width=\linewidth]{figs/husky.jpg}
    \caption{\label{fig:robot}
    Provisional appearance of the \project robot that we will use to collect
    data. A tablet, displaying facial animations, is mounted on a robotic arm.
    It can freely orient its `gaze' and use expressive movements. The mobile
    base (a Segway Husky) can autonomously navigate in the various parts of the
    BRL open-space}
\end{wrapfigure}

Designing behaviours that
enable sustained, long-term engagement in a social human-robot interaction is
essentially an open research question. Three main approaches to social
behaviours generation exist today: \emph{user-induced}, where the end-user
interacts with the robot and ascribes (knowingly or not) complex behaviours to
the machine, while in reality the robot's behaviours are simple and non-goal
oriented (eg generating a noise or a small movement when being touched). This
has been used to great effect in therapy robots, for instance (eg Paro).
\emph{Off-the-shelf behaviours}, where the robot relies on a set library of
behaviours (that might be individually relatively complex). The approach can
elicit a strong initial social response from the user, but this social response
tends to vanish rapidly once the 'tricks' of the robot have been all discovered
and become repetitive.  Besides, as the robot does not typically maintain a
long-term socio-cognitive plan of the interaction, the behaviours are typically
perceived as fun, yet pointless. This is often observed in toy-like robots (eg
Vector, Dot \& Dash). Finally, many social robots avoid altogether the problem
of generating behaviours by simply offering to the end-user control over
\emph{low-level behaviours} (eg, control of the joints of the robot). This means
that, even when the robot has relatively powerful social perception capabilities
(like recognising people and voice), no real social behaviours is generated.

None of these three approach is satisfactory, and indeed, no approach to date
has been able to engage human users in long-term, sustained interactions.




At a time where companion robots are coming to the market, one important
question remains fully open: how to design robot behaviours that foster
lasting engagement? A vast body of academic literature identifies that
robots evoke an initial phase of high user engagement (the
\emph{novelty} phase) that vanishes as the user realises that the robot
is actually quite predictable and repetitive. The \emph{agency}
initially ascribed by the user to the robot quickly
fades~\cite{lemaignan2014dynamics}, leading to critical user disengagement from the technology.


The (often limited) library of behaviours available to the robot is
often cited as a key factor in causing this issue. However, another,
more profound issue affecting long term engagement with robot companions
is the question of \emph{purpose.} Without clear \emph{purpose}, social
robot companions can lack \emph{usefulness}. Indeed, robot
\emph{companions} might not have explicit goals that would dictate or
motivate their behaviours: they aim at providing a social presence, a
social comfort, as cats or dogs would do, without necessarily being
goal-oriented.

Recent attempts -- and failures -- to convert social robotics research
into commercial platforms (Jibo, Kuri and most recently Anki's Cozmo and
Vector robots) reflect exactly this, with reasons for their failure
typically citing an under-delivery of the user experience they promised,
and/or the lack of a `real need' to justify their price point. The
\project project addresses these two key issues by:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Taking inspiration from human-pet relationships which also have no
  explicit \emph{purpose} beyond their potential for enjoyable,
  \emph{affective} interactions;
\item
  Working with creative professionals who excel at storytelling and
  emotional engagement to overcome the problems in sustaining
  engagement, as proposed by Hoffman\footnote{\url{https://spectrum.ieee.org/automaton/robotics/home-robots/anki-jibo-and-kuri-what-we-can-learn-from-social-robotics-failures}};
\item
  Blending these two sources of inspiration using a radically novel
  combination of immersive teleoperation and machine learning.
\end{enumerate}

The project is \emph{not} about replicating a pet's behaviour per se. It
is instead about identifying, modeling and automatically generating the
social behaviours required to recreate pet-like social dynamics between
robots and humans, drawing inspiration from ethology (Stanton, Sullivan,
and Fazio 2015). Using animal behaviours to inform the design of robots
is not new, the most remarkable example being the Sony AIBO robot dog,
whose behaviours were directly designed around those of actual dogs
(Arkin et al. 2003). However, to go beyond the repetitive interactions
associated with such robots, we propose to employ a creative
professional to actively participate in design and automation of \project
behaviour. The concept of using creative professionals to `teach' social
robot behaviour is not new either (Knight and Gray 2012), however it is
only recent advances in human-in-the-loop, online machine learning that
make this type of real-time `social training' a feasible approach to
generating and automating engaging social behaviours (Senft et al.
2019).

Our project has the following goals, addressed by the workplan presented
below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  assemble a non-anthropomorphic social robot that can autonomously
  navigate in a complex and living lab environment, taking inspiration
  from ethology to inspire the robot's behaviour;
\item
  develop an immersive teleoperation system, enabling a creative
  professional to `take control' of the robot (i.e.~puppet the robot) in
  a completely intuitive way (using whole body motion tracking);
\item
  record (and make publicly available) a large dataset of social
  behaviours (created through immersive teleoperation) that foster
  long-term social and affective engagement. The dataset will also
  include the social \emph{signals} implicitly used by the puppeteer to
  drive his/her choice of actions (recorded through eg eye-tracking);
\item
  using machine learning, map these social signals (input state) to the
  robot behaviours (output state) such that the robot can operate
  autonomously.
\end{enumerate}

A creative professional (puppeteer, dancer or comedian --
corresponding financial compensation is budgeted) will join the group.
First she/he will take part to a one-week co-design workshop (4) aiming
at finalising the immersive teleoperation controller and the behaviours
of the robot. Then, she/he will interact for about 4 hours a day during
a month, with the BRL lab members (200+ researchers). She/he will do so
by remotely operating the robot (5) from an (out-of-sight) control room
(the BRL CAVE room). The aim will be for the puppeteer to pro-actively
engage with people in the lab, attempting to engage in \emph{social,
affective} interactions. This will be achieved by creating/inventing
in-situ a new `grammar' of social behaviour, loosely inspired by those
of cats and other pets. These interactions will be fully recorded
(including eye-tracking on the puppeeter) (6), in order to create a
unique dataset of complex social interactions, suitable for machine
learning. The PI has already extensive experience in recording such
datasets (see (Lemaignan et al. 2018) for instance).

%\begin{wrapfigure}[17]{l}{8cm}
\begin{figure}
    \centering
    \includegraphics[width=0.8\paperwidth]{figs/dev.pdf}
    \caption{\label{fig:support}
    Social behaviours will be learned from immersive `puppetering' of the
    robot, performed by a professional actor. The `puppetering' takes place
    in a CAVE (or VR) environment, where what the robot `sees' and `hears'
    is streamed live}
\end{figure}
%\end{wrapfigure}

Over the following four months, a deep neural network will be designed
and trained (7) for the regression task of generating continuous social
behaviours from perceived social signals. In parallel, a software
controller will be developed (8) to enable generic autonomous
capabilities (like autonomous navigation) for which the BRL has
extensive expertise.

Finally, the last four months will be dedicated to in-situ testing of
the autonomous system (9). We will seek to conduct a large scale study
within the lab, over a period of several weeks. For this study, the
robot is expected to be fully autonomous. However sufficient amount of
time is planned for additional iterations on the development of the
robot controller if deemed necessary. We aim at publishing the results
of this main study shortly after the end of the one-year period.


\subsection{T4.3: Non-verbal behaviours and robot soundscape}

\begin{figure}[!htbp]
\centering
    \includegraphics[width=0.7\textwidth]{figs/cozmo-expression-sheet.jpg}
\caption{Cozmo facial expressions}
\end{figure}


In task T4.3, we
introduce a novel non-verbal interaction modality for robots, based on
soundscapes: soundscapes are about creating a sound environment that reflects a
particular situation; they also have been shown to be an effective intervention
technique in the context special need treatments
(eg~\cite{greher2010soundscape}). The soundscapes that we will create, are
`owned' by the robot, and it can manipulate it itself, eg to create an
approachable, non-threatening, non-judgmental, social interaction context, or to
the establish the interaction into a trusted physical and emotional safe-space
for the children.

\textbf{Specific resource}: these soundscapes will be co-designed with Dr.
Dave Meckin, an expert on sound design for vulnerable children, who also works
at the host institution.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{WP5: \textbf{\wpFive}}

\noindent\framebox[\linewidth]{Duration: \textbf{Y2-Y5}; one post-doc (shared
with WP3)}





Critically, the scientific investigation of these questions (\emph{conceptual
framing of robot-supported human-human interaction}, \emph{socially-driven
cognitive architecture},  \emph{online social situation assessment and social
behaviour generation}) has to be
grounded in real-world applications. Indeed, two complex, high impact
applications are detailed in the project. The first one takes place in
Bristol's Children Hospital: a social robot will be permanently deployed in one
of ward, supporting isolated children who suffer long-term conditions, in close
cooperation with the hospital staff. The second one involves the deployment of 
social robots in special need schools (SEN schools) in Bristol. Building on a
rigorous participatory approach involving the school teachers, as well as the
parents, we will seek to integrate the robot in the daily life of the school,
supporting the development of the students' physical and social skills.

These two applications, detailed hereafter, aim at supporting
socially-vulnerable populations: this entails specific risks (detailed and
addressed in the Section~\ref{risks}), but also offers a unique opportunity to
demonstrate in a real-world, impactful case, how social robots can effectively
have a positive, ethical impact, and support in-fine stronger, richer
human-human interactions.


\project aims at developing and demonstrating the need for a global approach to
social HRI with ambitious, high-impact, socially meaningful, applications.

The two last application scenarios are ambitious and inherently risky, as they
target vulnerable populations. This is an however informed choice: first, we
already have established partnerships with Bristol's children hospital on one
hand, and a network of Bristol-based SEN schools on the other hand. As such, and
from a practical perspective, we do not foresee any institutional issues -- on
the contrary, our partners are excited at the prospect of taking part to the
project. Besides, convincingly demonstrating the importance and positive impact
of socially-driven, socially-responsible robotics does accordingly require
complex social situations, and complex social dynamics. The two scenarios, which
complement each other, provide both. These scenarios also put the project in the
unique position of actually delivering high societal impact: we anticipate 100+
hospitalised children with long-term conditions, and 50+ SEN-educated children
to directly benefit of the project, showing how robots can have a lasting,
strong, positive impact on the society, also establishing the idea of
\emph{robots supporting human interactions} instead of dehumanising our social
relationships.


\TODO{probably useful to give specific examples of interventions}


\subsection{Application 1: Social robots in SEN schools}

Can a socially assistive robot effectively support the development (learning?),
social interactions and well-being of children with a long-term mental
condition? Our project aim is an investigation of this question: how can a
social robot integrate into the eco-system of a SEN (Special Educational Needs)
school (specifically, the Mendip School, which welcomes pupils with ASD
(Autistic Spectrum Disorder), and effectively support the day-to-day work of the
school staff to support the development, learning and well-being of the
children. This project is naturally interdisciplinary as it requires expertise
from mental health/special needs education, classroom design and social
robotics. 

The main questions we seek to investigate are: What are the (social \& spatial)
underpinnings of the successful integration of a social robot in the school
ecosystem? Can ambitious co-design with the end-users (teachers) deliver a 'net
gain' for the learning, social interaction and well-being of the students? 


The core of the project consists of deploying a social robot (likely one of the
BRL-owned Softbank Peppers) in a Bristol-based SEN school (the Mendip School),
to investigate how the robot can help shaping a spatial and social school
ecology that fosters mental well-being, while effectively supporting teachers
and students in their learning. 

The project will adopt a strong participatory design approach, with 2 one-day
workshops organised with the school teachers; and one evening workshop with the
school parents, prior to the school study (Jun./Jul. 2020). During the first
workshop, the teachers will be introduced to the robot capabilities with
examples of robot-supported teaching activities, and the robot's visual
programming interface will be introduced. We will also conduct group discussions
on how the robot can best be integrated in the daily school routine and
classroom context. During the second workshop, the teachers will be invited to
create novel activities, with the support of the research team. An evening focus
group will be organised as well with the parents, to integrate their
perspectives in the design of the robotic system.  Will we formally analyse the
data from this – will it become a research paper? 

Following the workshops, the teacher-oriented codesign of the robot's activities
and supervision tools (eg to start/stop/pause/resume activities) will be
finalised and implemented by the research team (Aug./Sep.2020). 

The school study itself will take place over 2 months (tentatively Oct./Nov.
2020), with the robot visiting the school once a week for the whole day. During
these visits, the robot will take part in the regular teaching and other daily
routines of the school, and will directly interact with the children. While the
robot's behaviours will have been co-designed with the teachers, the robot will
be teleoperated by a member of the research team, to ensure we do not create an
additional burden for the staff. 

During the 'robot days', observations will be conducted by the research team,
and regular semi-structured interviews will be conducted with the teachers,
parents, and where possible, the children themselves, to understand how the
robot impacts the school dynamics  (both positively and potentially negatively).
Is it appropriate/pertinent to do pre- and post- surveys of acceptance of robot? 

\textbf{Specific resources} this task will take place within a network of
Bristol-based SEN schools, with which I already have on-going collaborations
(child-robot interaction for children with autism at Bristol's Mendip School).
The task will be jointly supervised with local colleague and expert Dr.Nigel Newbutt,
who has a long track record of working with special needs schools.





\subsection{Application 2: A robot companion at the children's hospital}
\textbf{T5.2 -- creation and deployment of a small robot companion to support
isolated children during their hospital stay}, fully integrated and aware of the
wider hospital ecosystem. Over the course of this second, one-year long (Y4)
experiment, we will deploy one \project robot at the Bristol Children Hospital.
Using a \emph{mutual shaping} approach~\cite{winkle2018social} to design the
role of the robot with the different stakeholders (nurses, doctors, parents,
children), we will experimentally investigate how a social robot can support
hospitalised children with long-term conditions. The robot's role will revolve
around facilitating social interactions between possibly socially isolated
children, by fostering playful interaction with a yard.

\textbf{Specific resources} this task will take place at the Bristol Children
Hospital. Several preparatory meetings already took place with the head of the
hospital education service J. Bowyer, who will support the project, giving me
access to two of the long-term conditions wards for the duration of the studies.



\TODO{explain that these 2 large experiments will be scaffolded by many smaller
ones}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Ethics consideration and measures to ensure Responsible Research and Innovation (RRI)}



One example: a common (ethics-related) objection to human-robot interaction is
the perceived deceptive nature of the robot's role. As recently
argued~\cite{biscontilucidi2018companion}, the underlying concern is likely
instead the need to simulate of a human-like interaction with robots, in the
absence of an adequate (and novel) model of human-robot interactions to refer
to.


BS 8611~\cite{bsi2016robots}
~\cite{stahl2018implementing}


\TODO{taken from Stahl2018; rephrase}

The EC has adopted RRI as a cross-cut-
ting activity in its Horizon 2020 research framework programme.

For the purposes of this paper, I will use the concept of RRI as put developed
by~\cite{stilgoe2013developing} and subsequently adopted by the UK Engineering
and Physical Sciences Research Council~\cite{owen2014uk}. This concept
represents RRI using the acronym AREA, which stands for anticipation,
reflection, engagement and action. A piece of research or innovation activity,
in order to count as having been undertaken responsibly, would need to
incorporate anticipation about possible consequences, integrate mechanisms of
reflection about the work, its aims and purposes, engage with relevant
stakeholders and guid action of researchers accordingly.

AREA 4P
Framework~\cite{stahl2018implementing}~\footnote{\url{https://www.orbit-rri.org/about/area-4p-framework/}}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newpage
\section{Risk/gain assessment; risk mitigations}\label{risks}

\textbf{Tasks 1.1, 1.2} develop a novel methodology, 'public-in-the-loop' machine
learning, for large-scale co-design of social interactions with the public. If
successful, this will be of great value, well beyond the project. The
proposed experimental setup (museum visitors 'taking control' of the robot)
might however lead to interactions that are either too short or to artificial to
create meaningful, generalisable social interaction. In addition, the messy and
complex nature of the museum environment is also currently beyond-state-of-art
in term of extracting the useful social features required to train a classifier.

However, the interaction principles that we want to uncover in T1.1 and T1.2
(and that are feeding into WP2 and WP3) will principally come from a qualitative
analysis of the interactions, carried in parallel to the machine learning
approach. This well within the expertise of the PI, and, as such, is low-risk.
T1.1 can thus be described as a \ul{\bf medium-risk, high-gain} component of
\project.

\vspace{1em}

\textbf{Task 2.1} develops a novel situation assessment component, that
integrates spato-temporal modeling with knowledge representation. The resulting
component is beyond-state-of-art, and would be highly relevant to a large range
of robotic applications. This component relies on integrating tools that are
independently relatively mature and well understood, and the principles of the
integration itself is already well researched. Besides, it falls well within the
PI
expertise~\cite{lemaignan2018underworlds,sallami2019simulation,lemaignan2010oro}.
As such, T2.1 can be described as \ul{\bf low-risk, medium-gain}.

\textbf{Tasks 2.2, 2.3, 2.4} Work on real-time modeling of social dynamics in
real-world environments are only begining to be studied in robotics. While the
underpinning are well understood in neighbouring academic fields, a very
significant work remain to be done to integrate disparate or partial approaches
into one framework. These tasks also require the acquisition of novel datasets
that focus on natural human-human social interactions. The PI has extensive
experience in building and acquiring such
datasets~\cite{lemaignan2018pinsoro,sallami2020unexpected}, and does not
foreseen major difficulties. The resulting components have however the potential
to unlock a new class of social robots, aware in real-time of their social
surroundings and dynamics.  These tasks are thus considered \ul{\bf low-risk,
high-gain}.

\vspace{1em}

\textbf{Task 3.1} The conceptual framing of a \emph{socially-driven
architecture} (social teleology) and its translation into decision-making
algorithms are to a large extend open questions. This task might however lead to
uncover a fundamental mechanism to enable long-term engagement of users
with social robots. Building fundamentally on blue-sky research, this task is
\ul{\bf high-risk, high-gain}. If not successful, I will instead rely on the
decision-making strategy of T3.2, which is much lower risk.

\textbf{Task 3.2} The techniques developed in T3.2 have been previously used and
tested by the PI in two different real-world
environments~\cite{senft2019teaching,winkle2020couch}. While they will require
significant adjustments for this project, the task is overall \ul{\bf low-risk,
low-gain}.

\textbf{Task 3.3} The integration of the different cognitive functions of the
robot into one principled cognitive architecture, that include cognitive
redundancy, is one of the core expertise of the
PI~\cite{lemaignan2017artificial}. This task however includes significant novel
elements (cognitive mechanisms for long-term autonomy; decision arbitration)
that bear unknowns. Besides, this task is a critical pre-requisite for WP5. As a
result, T3.3 is considered as \ul{\bf high-risk}. As the task is focused on
integration to meet the requirements of the WP5 experiments, significant parts
of the resulting software architecture might be project-specific, and as a
result, T3.3 is considered \ul{\bf low-gain}. The main mitigation comes from the
iterative development process of the architecture, that will start from the existing
state-of-art, to which the PI has previously
contributed~\cite{lemaignan2017artificial}. By doing so, a decisional
architecture for the robot will be available early on in the project. While that
architecture might be a scaled-down version of the initial ambition, it will
still enable the fieldwork proposed in WP5, possibly with a lesser level of
autonomy.

\vspace{1em}

\textbf{Task 4.1} The behavioural baseline implements the current state-of-art,
and as such is \ul{\bf low-risk, low-gain}. T4.1 will guarantee early on in the
project a 'working' robot, yet with predictable/repetitive behaviours.

\textbf{Task 4.2} The continuous generation of complex social behaviours is a
\ul{\bf high-risk, high-gain} task: while it builds on a well-established
state-of-art, it relies on very significant progress in both the modeling of the
social dynamics (WP2) and the capacity of designing a machine learning approach
to learn and generate these complex behaviours. While the former falls well
within the PI expertise, machine learning for social motion generation does not
lay within his core expertise. The sucess of this task will rely to a large
extend on the quality of the post-doctoral researcher recruited to lead this
effort. The main mitigation to the risk associated to T4.2 is the behavioural
baseline created in T4.1: the behavioural capabilities generated in T4.2 will be
complemented by ad-hoc behaviours were required.

\textbf{Task 4.3} Non-verbal communication is a well established subfield of HRI
research, well known to the PI. The creation of the novel interaction modality
based on soundscape is novel, with potential for impact beyond the project. This
new modality will be co-developped with an expert of sound design for
interaction, and we do not foresee major risks. Overall, the task is \ul{\bf
low-risk, medium-gain}.

\vspace{1em}

\textbf{WP5: Experimental deployments}

The two application scenarios (at the children hospital and in the SEN school)
are ambitious and inherently risky, as they target vulnerable populations.
However, first, demonstrating the importance of advanced social modelling, and
convincingly proving the effectiveness of our approach does require accordingly
complex social situations, and complex social dynamics. The two scenarios, which
complement each other, provide both.

Second, working with vulnerable populations, in constrained and complex
environments (children hospital and SEN schools) adds significant risks to the
project. But it is also what make the project in the unique position of
delivering a high societal impact: a direct positive impact on children's lives
(we anticipate 100+ hospitalised children and 50+ children with psycho-social
impairements interacting over long periods of time with a robot over the course
of the project), and a broader impact on the society, showing how robots can
have a lasting, strong, positive impact on the society, also establishing the
idea of \emph{robots supporting human interactions} instead of dehumanising our
social relationships.

\textbf{Together, Task 5.1 and 5.2 are \ul{high-risk, high-gain}.}

The two main mitigations are (1) early and continuous engagement with the
stakeholders, and (2) the decoupling of the two applications, meaning that the
risks associated to each of them do not impact the other one.

Early engagement will be ensured by relying on a participatory design
methodology, involving all the stakeholders from the onset of the project; the
methodology will involve regular joint workshops; on-site (hospital and refugee
camps) research stay including engagement with the staff/charities and the
children themselves; use of art-based participatory techniques like puppetering.
I will also perform field testing early on in the project, relying if necessary
on provisional, yet well-known, robot platforms available at the host
institution (for instance, Softbank Nao and Pepper). This user-centered approach
will be championed by the post-doc recruited on the project on WP3 and WP5, who will
have to have a strong expertise in user-centered design.

The PI, Pr. Séverin Lemaignan, has been working for 12+
years in human-robot interaction, and over the last 6 years, specifically in the
field of child-robot interaction.  His profile is both highly technical, with
hundreds of hardware and software contributions to the worldwide robotic
community; and highly experimental, running dozens of studies and experiments
with children, including long-term ones, in multiple schools and in healthcare
environments. He is in a unique position to deliver both a scientific
breakthrough on social situation assessment for robots, and a high-impact
societal change.

%\begin{table}[!htbp]
%\caption{Identified risks and proposed mitigations}
%\centering
%\begin{tabular}{@{}llll@{}}
%\toprule
%    \textbf{Task} & \textbf{Description of risk}                 & \textbf{Level} & \textbf{Proposed risk mitigation measures} \\ \midrule
%    T1.1          & Visitors' interactions with robot too short to create
%    meaningful social interaction                            & High
%    & The 'public-in-the-loop' machine learning methodology is high risk/high
%    gain; machine                            \\
%                  &                              &                &                            \\
%                  &                              &                &                            \\
%                  &                              &                &                            \\ \bottomrule
%\end{tabular}
%\end{table}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newpage
\printbibliography

\newrefsection
\newpage
\chapter{B2.c Description of resources}

\eu{Has to be provided online; 8000 chars max (eg 2 pages)}


\section{Requested budget}

\TODO{This table does not count toward the page limit and is to be provided online}
\begin{table}[]
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Labour cost}                                             & \textbf{} & \textbf{€ 1,413,344} \\ \midrule
Severin Lemaignan                                                & 36        & € 287,455            \\
RF 1 (sociology/anthropology, WP1)                               & 36        & € 195,779            \\
RF 2 (social signal processing/machine learning, WP2)            & 48        & € 263,722            \\
RF 3 (cognitive architecture for robotics, experiments, WP3 WP5) & 60        & € 333,050            \\
PhD 1 (social signal processing/machine learning, WP3 WP5)       & 42        & € 64,233             \\
RF 4 (behaviour generation, WP4)                                 & 48        & € 269,105            \\
                                                                 &           &                      \\
\textbf{Travel}                                                  & \textbf{} & \textbf{€ 65,865}    \\ \midrule
Conferences (1 per person per year): Registration                &           & € 22,910             \\
Conferences (1 per person per year): Travel                      &           & € 42,955             \\
                                                                 &           &                      \\
\textbf{Equipment}                                               & \textbf{} & \textbf{€ 334,527}   \\ \midrule
R1 Robot (inc VAT)                                               &           & € 292,800            \\
R1 Shipping                                                      &           & € 7,000              \\
Robot Installation and Training                                  &           & € 3,800              \\
Workstation suitable for machine learning                        &           & € 22,910             \\
Sensors (inc RGB-D cameras, one eye tracker)                     &           & € 8,018              \\
                                                                 &           &                      \\
\textbf{Materials}                                               & \textbf{} & \textbf{€ 11,455}    \\ \midrule
Other Consumables                                                &           & € 11,455             \\
                                                                 &           &                      \\
\textbf{Other}                                                   & \textbf{} & \textbf{€ 23,455}    \\ \midrule
Open access fees (2 Article per year)                            &           & € 12,000             \\
Participants compensations                                       &           & € 3,436              \\
Data Storage                                                     &           & € 2,291              \\
Cloud Computing                                                  &           & € 5,727              \\
                                                                 &           &                      \\
Audit                                                            &           & € 5,000              \\
                                                                 &           &                      \\
                                                                 &           &                      \\ \midrule
\textbf{Total}                                                   & \textbf{} & \textbf{€ 1,853,645} \\
\textbf{Indirect}                                                & \textbf{} & \textbf{€ 463,411}   \\
\textbf{EC Contribution}                                         & \textbf{} & \textbf{€ 2,317,056} \\ \bottomrule
\end{tabular}
\end{table}

\section{Research team and PI commitment}

Table~\ref{time-allocation-team} provides an overview of the time allocation per
members of the team, over the course of the project.

\begin{table}[h!]
    \centering
\begin{tabular}{@{}lccccccr@{}}
\toprule
\textit{\textbf{}}              & \textbf{Y1} & \textbf{Y2} & \textbf{Y3} & \textbf{Y4} & \textbf{Y5} &  & \textbf{Total months} \\ \midrule
\textit{Séverin Lemaignan (PI)} & 0.6         & 0.6         & 0.6         & 0.6    & 0.6         &  & 36                    \\ \midrule
\textit{Post-doc 1 (WP1)}       & 1           & 1           & 1           &             &             &  & 36                    \\
\textit{Post-doc 2 (WP2)}       & 1           & 1           & 1           & 1           &             &  & 48                    \\
\textit{Post-doc 3 (WP3, WP5)}  & 1           & 1           & 1           & 1           & 1           &  & 60                    \\
\textit{PhD 1 (WP3, WP5)}       &             & 1           & 1           & 1           & 0.5         &  & 42                    \\
\textit{Post-doc 4 (WP4)}       &             & 1           & 1           & 1           & 1           &  & 48                    \\ \bottomrule
\end{tabular}
    \caption{Full-time equivalent for the research team members}
    \label{time-allocation-team}
\end{table}

PI Séverin Lemaignan will dedicate 60\% (3 days/week) of his time to the
project. This time will cover significant research time (about 2 days/week) as
well as the supervision of the team and management of the project (1 day/week).

The rest of his time will be dedicate to other academic commitments within the
Bristol Robotics Lab (including the on-going supervision of his other PhD
students, supervision of MSc students, the supervision of the Human-Robot
Interaction research group at BRL, lab-wide strategic engagement), as well as a
small proportion of Master-level teaching in Human-Robot Interaction (about 5
days/term).

Each of the project work packages will have one lead researcher (senior
post-doc); the duration of each of the post-docs' contracts roughly matches the
duration of the corresponding work packages.

\begin{itemize}
    \item \textbf{WP1}: I will appoint a post-doc with a background in sociology
        of technology and science facilitation; the researcher will work for
        three year to frame the \emph{robot-supported human-human interactions}
        paradigm, and lead the field work at the WeTheCurious museum;

    \item \textbf{WP2}: WP2 will be led by a post-doc with a background in
        social signal processing and/or machine learning; the researcher will be
        appointed for 4 years; extensive collaboration with WP1's post-doc is
        expected to frame the social dynamics fostered by the robot;

    \item \textbf{WP3}: WP3 (the cognitive architecture) lays at the core of the
        project; the WP3 leader will be a senior post-doc in cognitive robotics,
        appointed for the whole 5 years to ensure continuity on this critical
        part; she/he will be responsible for the integration of the outputs of
        the other work packages; the same post-doc will also oversee (with the
        PI) the experimental work taking place in WP5. A PhD student will also
        be recruited on WP3, with a specific focus on the cognitive architecture
        design;

    \item \textbf{WP4}: one post-doc (background in learning from demonstration
        and machine learning) will be in charge of developing the novel
        continuous robot behaviour generation method, and will be appointed for
        4 years, starting on the second year.

\end{itemize}
\section{Research equipement}

I will purchase 2 R1 robots for the \project project. The R1 robot is a
recently developped service robot from the Italian Institute of Technology
(IIT). Figure~\ref{r1-specs} summarises the main features of the robot.

While the host institution (the Bristol Robotics Lab) could provide access to a
range of social robots, none of the currently available robots are suitable for
the project.  Table~\ref{robot-comparison} compares the main R1's feature with
those of PAL TiaGo and SoftBank Pepper, the two main other platforms that would
be potentially suitable for the project. However, neither TiaGo nor Pepper have
non-verbal social features that are powerful enough to deliver the \project
project. Critically, they both lack the abilities to show facial expressions or
simulated gazing behaviours. Because the R1 robot features a programmable
display in place of the head, we will have full freedom to create complex
non-verbal facial expressions.

\begin{table}[]
    \begin{tabular}{@{}p{3cm}p{5cm}p{5cm}p{5cm}@{}}
\toprule
                                           & PAL TiaGo                                                & Softbank Pepper                                                           & \textbf{IIT R1}                                                                                                                              \\ \midrule
\textbf{Social features}                   & Poor (non-expressive head)                               & Medium (expressive, yet fixed, face; no gaze; non-threatening appearance) & Good (expressive face~\cite{lehman2016head}; artificial skin for touch-based interactions; non-threatening appearance) \\
\textbf{Perception}                        & Medium (RGB-D camera; laser scanner; no microphone)      & Medium (RGB-D; simple mic array; poor laser scanner)                      & \TODO{check once datasheet}                                                                                                   \\
\textbf{Navigation}                        & Good (however, limited agility due to large footprint)   & Poor (weak localisation capabilities)                                     & \TODO{check once datasheet}                                                                                                   \\
\textbf{Safety}                            & Medium (heavy robot; large footprint; non-compliant arm) & Medium (smaller footprint; safe arms; limited stability)                  & Good (smaller footprint; safe arms; dynamic stability)                                                                                       \\
\textbf{Suitability for care environments} & Poor (relatively large, difficult to clear)              & Good (smaller footprint, easy to clean)                                   & Good (smaller footprint, easy to clean)                                                                                                      \\
\textbf{Manipulation capabilities}         & Medium (non-anthropomorphic gripper; single arm)         & Limited (poor gripper with low payload; dual arm)                         & Good (anthropomorphic gripper; pressure sensors; dual arm; 1.5kg payload)                                                                    \\ \bottomrule
\end{tabular}
    \label{robot-comparison}
\end{table}

\section{Open access}

In line with the European requirements, all journal publications will made
available under an Open Access license. On the basis of an average of 2 journal
publications per annum, and an average processing fee of €1,200 per article, we
request €12,000 to support Open Access costs. Note that conference publications
do not always offer immediate open-access policies.

\section{Existing resources available to the researcher}

The fellowship will take place at the Bristol Robotics Laboratory (BRL).  The
BRL is the largest co-located and most comprehensive advanced robotics research
establishment in the UK. It is a joint venture between the University of the
West of England and the University of Bristol. BRL's multidisciplinary approach
aims to create autonomous devices capable of working independently, with each
other, or with humans. BRL draws on robotics, electrical \& mechanical
engineering, computer science, psychology, cognitive science and sociology. BRL
has an international reputation as a leading research centre in advanced
robotics research and has over 250 researchers working on a broad portfolio of
topics: HRI, collective robotics, aerial robotics, neuro-inspired control,
haptics, control systems, energy harvesting and self-sustaining systems,
rehabilitation robotics, soft robotics and biomedical systems. BRL has many
collaboration partnerships, both national and international, and is experienced
in managing large multi-site projects. BRL has support from two embedded units
specialising in business and enterprise, together with an incubator and
successful track record of spin-outs.

Hardware-wise, the Bristol Robotics Lab, where this fellowship will take place,
offer unique support and facilities for robotic hardware development: the
laboratory's dedicated equipment includes two industrial-grade rapid prototyping
machine, a laser cutter, one 5-axis digital milling machine, all the required
facilities for PCB prototyping, and a team of six full-time technicians,
specialised in hardware development. The BRL has indeed a long track-record of
designing and building new and original robots (from the BERT humanoid in the
FP7 CHRIS project, to micro-robotics [in...?], to [...]). \project will directly
benefit of this expertise, which will ensure a feasible and realistic technical
implementation of the \project robots. Besides, an hardware engineer, dedicated
to the project, will be recruited in the frame of the project.

The BRL also include a hardware incubator and is co-located with 70 start-ups
and SMEs specialising in robotic hardware and mechatronics (Bristol's
\emph{FutureSpace}). This combination of excellent research and vast industry
expertise on one site is unique in the UK, and is will play an instrumental role
in providing a coherent and strong pathway to impact to the project, including
further engagement with industrial partners and spin-off opportunities.


\printbibliography


